{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from other_models.attention.attentive_pooling import AttentionWithContext\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data.pamap2.pamap2 import create_pamap2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle=False, finetuning=False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.finetuning = finetuning\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "        batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "        if not self.finetuning:\n",
    "            batch_x = window_slice(window_warp(batch_x))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    # epoch이 끝날때마다 실행\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(name, data_type=\"frequency_100\"):\n",
    "    print(\"creating dataset\")\n",
    "    if name == \"pamap2\":\n",
    "        x_train, y_train = create_pamap2(data=data_type)\n",
    "\n",
    "        # 80%, 10%, 10% respectively in train, val, test dataset\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=time_int, stratify=y_train)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=time_int, stratify=y_test)\n",
    "        print(\"x_train.shape : \", x_train.shape, \"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_val.shape   : \", x_val.shape,    \"y_val.shape: \", y_val.shape)\n",
    "        print(\"x_test.shape  : \", x_test.shape,   \"y_test.shape: \", y_test.shape)\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state :  4217067235\n"
     ]
    }
   ],
   "source": [
    "secs = time.time()\n",
    "tm = time.localtime(secs)\n",
    "\n",
    "time_string = time.strftime('%Y%m%d%I%M%S', tm)\n",
    "time_int = int(time_string) % (2**32 - 1)\n",
    "time_string = str(time_int)\n",
    "print('random_state : ' , time_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    \n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n",
    "            scale = (x.shape[1]-1)/time_warp[-1]\n",
    "            ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n",
    "    return ret\n",
    "\n",
    "def permutation(x, max_segments=5, seg_mode=\"equal\"):\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    \n",
    "    num_segs = np.random.randint(1, max_segments, size=(x.shape[0]))\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        if num_segs[i] > 1:\n",
    "            if seg_mode == \"random\":\n",
    "                split_points = np.random.choice(x.shape[1]-2, num_segs[i]-1, replace=False)\n",
    "                split_points.sort()\n",
    "                splits = np.split(orig_steps, split_points)\n",
    "            else:\n",
    "                splits = np.array_split(orig_steps, num_segs[i])\n",
    "            warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
    "            ret[i] = pat[warp]\n",
    "        else:\n",
    "            ret[i] = pat\n",
    "    return ret\n",
    "\n",
    "def window_slice(x, reduce_ratio=0.9):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n",
    "    if target_len >= x.shape[1]:\n",
    "        return x\n",
    "    starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n",
    "    ends = (target_len + starts).astype(int)\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n",
    "    return ret\n",
    "\n",
    "def window_warp(x, window_ratio=0.1, scales=[0.5, 2.]):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    warp_scales = np.random.choice(scales, x.shape[0])\n",
    "    warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n",
    "    window_steps = np.arange(warp_size)\n",
    "        \n",
    "    window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n",
    "    window_ends = (window_starts + warp_size).astype(int)\n",
    "            \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            start_seg = pat[:window_starts[i],dim]\n",
    "            window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n",
    "            end_seg = pat[window_ends[i]:,dim]\n",
    "            warped = np.concatenate((start_seg, window_seg, end_seg))                \n",
    "            ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n",
    "    return ret\n",
    "\n",
    "class DataAugLayer(tf.keras.models.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(DataAugLayer, self).__init__()\n",
    "        timestep, features = input_shape\n",
    "        self.seq_num, self.seq_length = 4, timestep//4\n",
    "        self.features = features\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = window_slice(time_warp(inputs))\n",
    "        x = tf.keras.layers.Reshape((self.seq_num, self.seq_length, self.features))(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Set(input_shape, kernel_size, name=\"Set\"):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=kernel_size, activation=\"relu\")\n",
    "    conv2 = tf.keras.layers.Conv1D(32, kernel_size=kernel_size, activation=\"relu\")\n",
    "    dropout = tf.keras.layers.Dropout(0.5)\n",
    "    maxpooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    x = tf.keras.layers.TimeDistributed(conv1)(inputs)\n",
    "    x = tf.keras.layers.TimeDistributed(conv2)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(dropout)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(maxpooling)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(flatten)(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, x, name=name)\n",
    "    return model\n",
    "\n",
    "def Encoder(input_shape, name=\"encoder\"):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    reshaped_inputs = tf.keras.layers.Reshape((4, 25, 18))(inputs)\n",
    "    branch_1 = CNN_Set((4, 25, 18), kernel_size=3, name=\"branch_1\")\n",
    "    branch_2 = CNN_Set((4, 25, 18), kernel_size=7, name=\"branch_2\")\n",
    "    branch_3 = CNN_Set((4, 25, 18), kernel_size=11, name=\"branch_3\")\n",
    "\n",
    "    one = branch_1(reshaped_inputs)\n",
    "    two = branch_2(reshaped_inputs)\n",
    "    three = branch_3(reshaped_inputs)\n",
    "    concat = tf.keras.layers.Concatenate()([one, two, three])\n",
    "    outputs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(concat)\n",
    "    model = tf.keras.models.Model(inputs, outputs, name=name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def add_projection_head(encoder, input_shape, contrastive_output):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = features[:, -1, :]\n",
    "    outputs = tf.keras.layers.Dense(contrastive_output, activation=\"relu\")(features)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"contrastive\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def Classifier(input_shape, encoder, n_outputs, trainable=True):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    encoded = encoder(inputs)\n",
    "    outputs =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False))(encoded)\n",
    "    x = tf.keras.layers.Dense(128)(outputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    classified = tf.keras.layers.Dense(n_outputs, activation=\"softmax\", name=\"classified\")(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=classified, name=\"Classifier\")     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset\n",
      "/home/tolom/20220621/contrastive_frequency\n",
      "x_train.shape :  (15502, 100, 18) y_train.shape:  (15502,)\n",
      "x_val.shape   :  (1938, 100, 18) y_val.shape:  (1938,)\n",
      "x_test.shape  :  (1938, 100, 18) y_test.shape:  (1938,)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"pamap2\"  \n",
    "x_train, y_train, x_val, y_val, x_test, y_test = create_dataset(name=dataset_name, data_type=\"nooverlap\")\n",
    "\n",
    "# contrastive_y_train = np.argmax(frequency_100_y_train, axis=1)\n",
    "# contrastive_y_val = np.argmax(frequency_100_y_val, axis=1)\n",
    "# contrastive_y_test = np.argmax(frequency_100_y_test, axis=1)\n",
    "\n",
    "# print(\"contrastive_y_train shape : \", frequency_100_y_train.shape)\n",
    "# print(\"contrastive_y_val shape : \", frequency_100_y_val.shape)\n",
    "# print(\"contrastive_y_test shape : \", frequency_100_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(x_train, y_train, 64, shuffle=True, finetuning=False)\n",
    "valid_loader = Dataloader(x_val, y_val, 64)\n",
    "test_loader = Dataloader(x_test, y_test, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  (100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 00:48:02.153676: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-28 00:48:05.019261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46722 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:d5:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"contrastive\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 100, 18)]         0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 4, 128)            395680    \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 128)              0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 412,192\n",
      "Trainable params: 412,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 00:48:18.275919: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n",
      "2022-06-28 00:48:26.796075: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 61s 165ms/step - loss: 3.3213 - val_loss: 3.0137\n",
      "Epoch 2/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.8876 - val_loss: 2.7975\n",
      "Epoch 3/35\n",
      "243/243 [==============================] - 38s 158ms/step - loss: 2.7091 - val_loss: 2.6716\n",
      "Epoch 4/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.6056 - val_loss: 2.5865\n",
      "Epoch 5/35\n",
      "243/243 [==============================] - 39s 155ms/step - loss: 2.5358 - val_loss: 2.5568\n",
      "Epoch 6/35\n",
      "243/243 [==============================] - 37s 153ms/step - loss: 2.4872 - val_loss: 2.5054\n",
      "Epoch 7/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.4446 - val_loss: 2.4799\n",
      "Epoch 8/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.4138 - val_loss: 2.4432\n",
      "Epoch 9/35\n",
      "243/243 [==============================] - 37s 153ms/step - loss: 2.3752 - val_loss: 2.4284\n",
      "Epoch 10/35\n",
      "243/243 [==============================] - 38s 155ms/step - loss: 2.3584 - val_loss: 2.4017\n",
      "Epoch 11/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.3290 - val_loss: 2.3945\n",
      "Epoch 12/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.3195 - val_loss: 2.3845\n",
      "Epoch 13/35\n",
      "243/243 [==============================] - 37s 154ms/step - loss: 2.2956 - val_loss: 2.3749\n",
      "Epoch 14/35\n",
      "243/243 [==============================] - 37s 154ms/step - loss: 2.2795 - val_loss: 2.3655\n",
      "Epoch 15/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.2679 - val_loss: 2.3526\n",
      "Epoch 16/35\n",
      "243/243 [==============================] - 37s 154ms/step - loss: 2.2510 - val_loss: 2.3467\n",
      "Epoch 17/35\n",
      "243/243 [==============================] - 38s 155ms/step - loss: 2.2472 - val_loss: 2.3349\n",
      "Epoch 18/35\n",
      "243/243 [==============================] - 37s 152ms/step - loss: 2.2320 - val_loss: 2.3369\n",
      "Epoch 19/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.2162 - val_loss: 2.3291\n",
      "Epoch 20/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.2017 - val_loss: 2.3191\n",
      "Epoch 21/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.2042 - val_loss: 2.3105\n",
      "Epoch 22/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.1857 - val_loss: 2.3154\n",
      "Epoch 23/35\n",
      "243/243 [==============================] - 38s 156ms/step - loss: 2.1756 - val_loss: 2.3104\n",
      "Epoch 24/35\n",
      "243/243 [==============================] - 38s 155ms/step - loss: 2.1705 - val_loss: 2.3044\n",
      "Epoch 25/35\n",
      "243/243 [==============================] - 37s 153ms/step - loss: 2.1676 - val_loss: 2.2985\n",
      "Epoch 26/35\n",
      "243/243 [==============================] - 38s 154ms/step - loss: 2.1497 - val_loss: 2.3052\n",
      "Epoch 27/35\n",
      "243/243 [==============================] - 38s 158ms/step - loss: 2.1449 - val_loss: 2.3059\n",
      "Epoch 28/35\n",
      "243/243 [==============================] - 38s 154ms/step - loss: 2.1425 - val_loss: 2.2839\n",
      "Epoch 29/35\n",
      "243/243 [==============================] - 38s 155ms/step - loss: 2.1310 - val_loss: 2.2979\n",
      "Epoch 30/35\n",
      "243/243 [==============================] - 39s 157ms/step - loss: 2.1277 - val_loss: 2.2799\n",
      "Epoch 31/35\n",
      "243/243 [==============================] - 38s 157ms/step - loss: 2.1117 - val_loss: 2.2792\n",
      "Epoch 32/35\n",
      "243/243 [==============================] - 39s 159ms/step - loss: 2.1143 - val_loss: 2.2702\n",
      "Epoch 33/35\n",
      "243/243 [==============================] - 37s 153ms/step - loss: 2.1150 - val_loss: 2.2758\n",
      "Epoch 34/35\n",
      "243/243 [==============================] - 37s 154ms/step - loss: 2.0996 - val_loss: 2.2831\n",
      "Epoch 35/35\n",
      "243/243 [==============================] - 38s 155ms/step - loss: 2.0980 - val_loss: 2.2676\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_shape = frequency_100_x_train.shape[1:]\n",
    "input_shape = (100, 18)\n",
    "print(\"input shape : \", input_shape)\n",
    "encoder = Encoder(input_shape)\n",
    "encoder_with_projection_head = add_projection_head(encoder, input_shape, 128)\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=SupervisedContrastiveLoss(0.5),\n",
    ")\n",
    "\n",
    "history = encoder_with_projection_head.fit(train_loader, validation_data=valid_loader, epochs=10)\n",
    "plot_training_loss(history, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataloader For Classifier finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.finetuning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prtraninig Encoder with Supervised Contrastive Loss]\n",
      "Model: \"Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 100, 18)]         0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 4, 128)            395680    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " classified (Dense)          (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 447,276\n",
      "Trainable params: 51,340\n",
      "Non-trainable params: 395,936\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "485/485 [==============================] - 16s 19ms/step - loss: 0.6519 - accuracy: 0.8400 - val_loss: 0.4987 - val_accuracy: 0.9025 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.2365 - accuracy: 0.9354 - val_loss: 0.3239 - val_accuracy: 0.9076 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.2028 - accuracy: 0.9447 - val_loss: 0.3029 - val_accuracy: 0.9159 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1888 - accuracy: 0.9457 - val_loss: 0.2926 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1755 - accuracy: 0.9497 - val_loss: 0.2863 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1668 - accuracy: 0.9515 - val_loss: 0.2872 - val_accuracy: 0.9159 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1560 - accuracy: 0.9536 - val_loss: 0.2816 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1562 - accuracy: 0.9545 - val_loss: 0.2818 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1554 - accuracy: 0.9552 - val_loss: 0.2823 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1424 - accuracy: 0.9580 - val_loss: 0.2758 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1415 - accuracy: 0.9577 - val_loss: 0.2775 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1425 - accuracy: 0.9587 - val_loss: 0.2790 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "485/485 [==============================] - 8s 15ms/step - loss: 0.1365 - accuracy: 0.9594 - val_loss: 0.2816 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1317 - accuracy: 0.9599 - val_loss: 0.2784 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1330 - accuracy: 0.9597 - val_loss: 0.2733 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1312 - accuracy: 0.9597 - val_loss: 0.2819 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1295 - accuracy: 0.9597 - val_loss: 0.2794 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1279 - accuracy: 0.9606 - val_loss: 0.2718 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1240 - accuracy: 0.9613 - val_loss: 0.2744 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1199 - accuracy: 0.9645 - val_loss: 0.2817 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1224 - accuracy: 0.9619 - val_loss: 0.2769 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1180 - accuracy: 0.9641 - val_loss: 0.2801 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "485/485 [==============================] - 6s 13ms/step - loss: 0.1191 - accuracy: 0.9639 - val_loss: 0.2725 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1137 - accuracy: 0.9654 - val_loss: 0.2828 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1134 - accuracy: 0.9646 - val_loss: 0.2863 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1146 - accuracy: 0.9650 - val_loss: 0.2924 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1124 - accuracy: 0.9642 - val_loss: 0.2756 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1104 - accuracy: 0.9660 - val_loss: 0.2789 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1107 - accuracy: 0.9664 - val_loss: 0.2816 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1086 - accuracy: 0.9669 - val_loss: 0.2733 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1085 - accuracy: 0.9661 - val_loss: 0.2709 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1037 - accuracy: 0.9679 - val_loss: 0.2863 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1050 - accuracy: 0.9672 - val_loss: 0.2791 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1040 - accuracy: 0.9672 - val_loss: 0.2850 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1030 - accuracy: 0.9668 - val_loss: 0.2935 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1048 - accuracy: 0.9671 - val_loss: 0.2826 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "485/485 [==============================] - 8s 17ms/step - loss: 0.1018 - accuracy: 0.9674 - val_loss: 0.2825 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1040 - accuracy: 0.9677 - val_loss: 0.2757 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1017 - accuracy: 0.9691 - val_loss: 0.2790 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1012 - accuracy: 0.9685 - val_loss: 0.2767 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1022 - accuracy: 0.9672 - val_loss: 0.2833 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1012 - accuracy: 0.9674 - val_loss: 0.2996 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1010 - accuracy: 0.9690 - val_loss: 0.2865 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0976 - accuracy: 0.9700 - val_loss: 0.2940 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0956 - accuracy: 0.9703 - val_loss: 0.2796 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0958 - accuracy: 0.9697 - val_loss: 0.2946 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0994 - accuracy: 0.9689 - val_loss: 0.2916 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0982 - accuracy: 0.9678 - val_loss: 0.2883 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0994 - accuracy: 0.9689 - val_loss: 0.2911 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0950 - accuracy: 0.9694 - val_loss: 0.2865 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0910 - accuracy: 0.9709 - val_loss: 0.2809 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0933 - accuracy: 0.9706 - val_loss: 0.2999 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0923 - accuracy: 0.9701 - val_loss: 0.2872 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0921 - accuracy: 0.9712 - val_loss: 0.2907 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0964 - accuracy: 0.9689 - val_loss: 0.2831 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0917 - accuracy: 0.9719 - val_loss: 0.2833 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0939 - accuracy: 0.9698 - val_loss: 0.2907 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0912 - accuracy: 0.9708 - val_loss: 0.3055 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0932 - accuracy: 0.9688 - val_loss: 0.2839 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.2943 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0922 - accuracy: 0.9687 - val_loss: 0.3051 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.2939 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0884 - accuracy: 0.9721 - val_loss: 0.2837 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0888 - accuracy: 0.9714 - val_loss: 0.2886 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0890 - accuracy: 0.9716 - val_loss: 0.2952 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0906 - accuracy: 0.9708 - val_loss: 0.2950 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.3068 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0885 - accuracy: 0.9717 - val_loss: 0.2899 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0888 - accuracy: 0.9706 - val_loss: 0.3047 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "485/485 [==============================] - 8s 17ms/step - loss: 0.0841 - accuracy: 0.9726 - val_loss: 0.2932 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0869 - accuracy: 0.9716 - val_loss: 0.3037 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0872 - accuracy: 0.9707 - val_loss: 0.2994 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0866 - accuracy: 0.9719 - val_loss: 0.2959 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0846 - accuracy: 0.9717 - val_loss: 0.3019 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0838 - accuracy: 0.9719 - val_loss: 0.3038 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0856 - accuracy: 0.9723 - val_loss: 0.3021 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0849 - accuracy: 0.9733 - val_loss: 0.3050 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0862 - accuracy: 0.9723 - val_loss: 0.2835 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0829 - accuracy: 0.9732 - val_loss: 0.3068 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0839 - accuracy: 0.9728 - val_loss: 0.2991 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0853 - accuracy: 0.9722 - val_loss: 0.2975 - val_accuracy: 0.9257 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0822 - accuracy: 0.9736 - val_loss: 0.3114 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0838 - accuracy: 0.9725 - val_loss: 0.2896 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0799 - accuracy: 0.9739 - val_loss: 0.3114 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0801 - accuracy: 0.9741 - val_loss: 0.3114 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0820 - accuracy: 0.9743 - val_loss: 0.3054 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 0.3098 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0812 - accuracy: 0.9728 - val_loss: 0.3086 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0820 - accuracy: 0.9726 - val_loss: 0.3280 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0814 - accuracy: 0.9745 - val_loss: 0.3089 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0801 - accuracy: 0.9740 - val_loss: 0.3141 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0821 - accuracy: 0.9727 - val_loss: 0.3105 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0804 - accuracy: 0.9734 - val_loss: 0.3246 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0808 - accuracy: 0.9720 - val_loss: 0.3380 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0789 - accuracy: 0.9745 - val_loss: 0.2991 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0827 - accuracy: 0.9723 - val_loss: 0.3082 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0809 - accuracy: 0.9742 - val_loss: 0.3107 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0782 - accuracy: 0.9734 - val_loss: 0.3204 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0779 - accuracy: 0.9745 - val_loss: 0.3018 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0809 - accuracy: 0.9719 - val_loss: 0.3004 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "[Finished Prtraining]\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "epcohs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "filepath = \"result/frequency_\" + dataset_name + \"_\" + time_string + \".h5\"\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=learning_rate*10)\n",
    "checkpoint = ModelCheckpoint(filepath, verbose = 0, monitor=\"val_loss\", mode=\"min\", save_best_only=True, save_weights_only = True)\n",
    "\n",
    "print(\"[Prtraninig Encoder with Supervised Contrastive Loss]\")\n",
    "model = Classifier(x_train.shape[1:], encoder, len(np.unique(y_train)), trainable=False)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "# history = model.fit(frequency_100_x_train, [frequency_100_y_train, contrastive_y_train], validation_data=(frequency_100_x_val, [frequency_100_y_val, contrastive_y_val]), batch_size=32, epochs=50, verbose=1, callbacks=[reduce_lr, checkpoint])\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch, epochs=epcohs, verbose=1, callbacks=[reduce_lr, checkpoint])\n",
    "print(\"[Finished Prtraining]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State :  4217067235\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'classified_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM 20220627.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRandom State : \u001b[39m\u001b[39m\"\u001b[39m, time_string)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m plot_training_acc(history, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m plot_training_loss(history, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mclf()\n",
      "File \u001b[0;32m~/20220621/contrastive_frequency/utils.py:26\u001b[0m, in \u001b[0;36mplot_training_acc\u001b[0;34m(H, plotPath)\u001b[0m\n\u001b[1;32m     24\u001b[0m plt\u001b[39m.\u001b[39mstyle\u001b[39m.\u001b[39muse(\u001b[39m\"\u001b[39m\u001b[39mggplot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m---> 26\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39;49mhistory[\u001b[39m\"\u001b[39;49m\u001b[39mclassified_accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_classified_accuracy\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTraining accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classified_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Random State : \", time_string)\n",
    "plot_training_acc(history, None)\n",
    "plot_training_loss(history, None)\n",
    "plt.clf()\n",
    "print_test_result(model, filepath, x_test, y_test, y_test, dataset=dataset_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d824acd22b2fc754dcfce7f06c4431e4c169f3c3962531c2898a757b0292919"
  },
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "ajh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
