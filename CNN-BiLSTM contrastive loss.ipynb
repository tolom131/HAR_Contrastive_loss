{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from other_models.attention.attentive_pooling import AttentionWithContext\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data.pamap2.pamap2 import create_pamap2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle=False, finetuning=False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.finetuning = finetuning\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "        batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "        if not self.finetuning:\n",
    "            batch_x = window_warp(batch_x)\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    # epoch이 끝날때마다 실행\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(name, data_type=\"frequency_100\"):\n",
    "    print(\"creating dataset\")\n",
    "    if name == \"pamap2\":\n",
    "        x_train, y_train = create_pamap2(data=data_type)\n",
    "\n",
    "        # 80%, 10%, 10% respectively in train, val, test dataset\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=time_int, stratify=y_train)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=time_int, stratify=y_test)\n",
    "        print(\"x_train.shape : \", x_train.shape, \"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_val.shape   : \", x_val.shape,    \"y_val.shape: \", y_val.shape)\n",
    "        print(\"x_test.shape  : \", x_test.shape,   \"y_test.shape: \", y_test.shape)\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state :  4216955842\n"
     ]
    }
   ],
   "source": [
    "secs = time.time()\n",
    "tm = time.localtime(secs)\n",
    "\n",
    "time_string = time.strftime('%Y%m%d%I%M%S', tm)\n",
    "time_int = int(time_string) % (2**32 - 1)\n",
    "time_string = str(time_int)\n",
    "print('random_state : ' , time_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    \n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n",
    "            scale = (x.shape[1]-1)/time_warp[-1]\n",
    "            ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n",
    "    return ret\n",
    "\n",
    "def permutation(x, max_segments=5, seg_mode=\"equal\"):\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    \n",
    "    num_segs = np.random.randint(1, max_segments, size=(x.shape[0]))\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        if num_segs[i] > 1:\n",
    "            if seg_mode == \"random\":\n",
    "                split_points = np.random.choice(x.shape[1]-2, num_segs[i]-1, replace=False)\n",
    "                split_points.sort()\n",
    "                splits = np.split(orig_steps, split_points)\n",
    "            else:\n",
    "                splits = np.array_split(orig_steps, num_segs[i])\n",
    "            warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
    "            ret[i] = pat[warp]\n",
    "        else:\n",
    "            ret[i] = pat\n",
    "    return ret\n",
    "\n",
    "def window_slice(x, reduce_ratio=0.9):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n",
    "    if target_len >= x.shape[1]:\n",
    "        return x\n",
    "    starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n",
    "    ends = (target_len + starts).astype(int)\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n",
    "    return ret\n",
    "\n",
    "def window_warp(x, window_ratio=0.1, scales=[0.5, 2.]):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    warp_scales = np.random.choice(scales, x.shape[0])\n",
    "    warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n",
    "    window_steps = np.arange(warp_size)\n",
    "        \n",
    "    window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n",
    "    window_ends = (window_starts + warp_size).astype(int)\n",
    "            \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            start_seg = pat[:window_starts[i],dim]\n",
    "            window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n",
    "            end_seg = pat[window_ends[i]:,dim]\n",
    "            warped = np.concatenate((start_seg, window_seg, end_seg))                \n",
    "            ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n",
    "    return ret\n",
    "\n",
    "# class DataAugLayer(tf.keras.models.Model):\n",
    "#     def __init__(self, input_shape):\n",
    "#         super(DataAugLayer, self).__init__()\n",
    "#         timestep, features = input_shape\n",
    "#         self.seq_num, self.seq_length = 4, timestep//4\n",
    "#         self.features = features\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         x = window_slice(time_warp(inputs))\n",
    "#         x = tf.keras.layers.Reshape((self.seq_num, self.seq_length, self.features))(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Set(input_shape, kernel_size, name=\"Set\"):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=kernel_size, activation=\"relu\")\n",
    "    conv2 = tf.keras.layers.Conv1D(32, kernel_size=kernel_size, activation=\"relu\")\n",
    "    dropout = tf.keras.layers.Dropout(0.5)\n",
    "    maxpooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    x = tf.keras.layers.TimeDistributed(conv1)(inputs)\n",
    "    x = tf.keras.layers.TimeDistributed(conv2)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(dropout)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(maxpooling)(x)\n",
    "    x = tf.keras.layers.TimeDistributed(flatten)(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, x, name=name)\n",
    "    return model\n",
    "\n",
    "def Encoder(input_shape, name=\"encoder\"):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    reshaped_inputs = tf.keras.layers.Reshape((4, 25, 18))(inputs)\n",
    "    branch_1 = CNN_Set((4, 25, 18), kernel_size=3, name=\"branch_1\")\n",
    "    branch_2 = CNN_Set((4, 25, 18), kernel_size=7, name=\"branch_2\")\n",
    "    branch_3 = CNN_Set((4, 25, 18), kernel_size=11, name=\"branch_3\")\n",
    "\n",
    "    one = branch_1(reshaped_inputs)\n",
    "    two = branch_2(reshaped_inputs)\n",
    "    three = branch_3(reshaped_inputs)\n",
    "    concat = tf.keras.layers.Concatenate()([one, two, three])\n",
    "    outputs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(concat)\n",
    "    model = tf.keras.models.Model(inputs, outputs, name=name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def add_projection_head(encoder, input_shape, contrastive_output):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    feautres = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(features)\n",
    "    outputs = tf.keras.layers.Dense(contrastive_output, activation=\"relu\")(features)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"contrastive\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def Classifier(input_shape, encoder, n_outputs, trainable=True):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable    \n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    encoded = encoder(inputs)\n",
    "    outputs =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False))(encoded)\n",
    "    x = tf.keras.layers.Dense(128)(outputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    classified = tf.keras.layers.Dense(n_outputs, activation=\"softmax\", name=\"classified\")(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=classified, name=\"Classifier\")     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset\n",
      "/home/tolom/20220621/contrastive_frequency\n",
      "x_train.shape :  (15502, 100, 18) y_train.shape:  (15502,)\n",
      "x_val.shape   :  (1938, 100, 18) y_val.shape:  (1938,)\n",
      "x_test.shape  :  (1938, 100, 18) y_test.shape:  (1938,)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"pamap2\"  \n",
    "x_train, y_train, x_val, y_val, x_test, y_test = create_dataset(name=dataset_name, data_type=\"nooverlap\")\n",
    "\n",
    "# contrastive_y_train = np.argmax(frequency_100_y_train, axis=1)\n",
    "# contrastive_y_val = np.argmax(frequency_100_y_val, axis=1)\n",
    "# contrastive_y_test = np.argmax(frequency_100_y_test, axis=1)\n",
    "\n",
    "# print(\"contrastive_y_train shape : \", frequency_100_y_train.shape)\n",
    "# print(\"contrastive_y_val shape : \", frequency_100_y_val.shape)\n",
    "# print(\"contrastive_y_test shape : \", frequency_100_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(x_train, y_train, 64, shuffle=True, finetuning=False)\n",
    "valid_loader = Dataloader(x_val, y_val, 64)\n",
    "test_loader = Dataloader(x_test, y_test, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  (100, 18)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# input_shape = frequency_100_x_train.shape[1:]\n",
    "input_shape = (100, 18)\n",
    "print(\"input shape : \", input_shape)\n",
    "encoder = Encoder(input_shape)\n",
    "encoder_with_projection_head = add_projection_head(encoder, input_shape, 64)\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=SupervisedContrastiveLoss(0.5),\n",
    ")\n",
    "\n",
    "history = encoder_with_projection_head.fit(train_loader, validation_data=valid_loader, epochs=10, batch_size=16)\n",
    "plot_training_loss(history, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataloader For Classifier finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.finetuning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prtraninig Encoder with Supervised Contrastive Loss]\n",
      "Model: \"Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 100, 18)]         0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 4, 128)            395680    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " classified (Dense)          (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 447,276\n",
      "Trainable params: 51,340\n",
      "Non-trainable params: 395,936\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "485/485 [==============================] - 16s 19ms/step - loss: 0.6519 - accuracy: 0.8400 - val_loss: 0.4987 - val_accuracy: 0.9025 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.2365 - accuracy: 0.9354 - val_loss: 0.3239 - val_accuracy: 0.9076 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.2028 - accuracy: 0.9447 - val_loss: 0.3029 - val_accuracy: 0.9159 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1888 - accuracy: 0.9457 - val_loss: 0.2926 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1755 - accuracy: 0.9497 - val_loss: 0.2863 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1668 - accuracy: 0.9515 - val_loss: 0.2872 - val_accuracy: 0.9159 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1560 - accuracy: 0.9536 - val_loss: 0.2816 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1562 - accuracy: 0.9545 - val_loss: 0.2818 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1554 - accuracy: 0.9552 - val_loss: 0.2823 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1424 - accuracy: 0.9580 - val_loss: 0.2758 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1415 - accuracy: 0.9577 - val_loss: 0.2775 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1425 - accuracy: 0.9587 - val_loss: 0.2790 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "485/485 [==============================] - 8s 15ms/step - loss: 0.1365 - accuracy: 0.9594 - val_loss: 0.2816 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1317 - accuracy: 0.9599 - val_loss: 0.2784 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1330 - accuracy: 0.9597 - val_loss: 0.2733 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1312 - accuracy: 0.9597 - val_loss: 0.2819 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1295 - accuracy: 0.9597 - val_loss: 0.2794 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1279 - accuracy: 0.9606 - val_loss: 0.2718 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1240 - accuracy: 0.9613 - val_loss: 0.2744 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1199 - accuracy: 0.9645 - val_loss: 0.2817 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1224 - accuracy: 0.9619 - val_loss: 0.2769 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1180 - accuracy: 0.9641 - val_loss: 0.2801 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "485/485 [==============================] - 6s 13ms/step - loss: 0.1191 - accuracy: 0.9639 - val_loss: 0.2725 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1137 - accuracy: 0.9654 - val_loss: 0.2828 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1134 - accuracy: 0.9646 - val_loss: 0.2863 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1146 - accuracy: 0.9650 - val_loss: 0.2924 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1124 - accuracy: 0.9642 - val_loss: 0.2756 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1104 - accuracy: 0.9660 - val_loss: 0.2789 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1107 - accuracy: 0.9664 - val_loss: 0.2816 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1086 - accuracy: 0.9669 - val_loss: 0.2733 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1085 - accuracy: 0.9661 - val_loss: 0.2709 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1037 - accuracy: 0.9679 - val_loss: 0.2863 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1050 - accuracy: 0.9672 - val_loss: 0.2791 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1040 - accuracy: 0.9672 - val_loss: 0.2850 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1030 - accuracy: 0.9668 - val_loss: 0.2935 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1048 - accuracy: 0.9671 - val_loss: 0.2826 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "485/485 [==============================] - 8s 17ms/step - loss: 0.1018 - accuracy: 0.9674 - val_loss: 0.2825 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1040 - accuracy: 0.9677 - val_loss: 0.2757 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.1017 - accuracy: 0.9691 - val_loss: 0.2790 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1012 - accuracy: 0.9685 - val_loss: 0.2767 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1022 - accuracy: 0.9672 - val_loss: 0.2833 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.1012 - accuracy: 0.9674 - val_loss: 0.2996 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.1010 - accuracy: 0.9690 - val_loss: 0.2865 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0976 - accuracy: 0.9700 - val_loss: 0.2940 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0956 - accuracy: 0.9703 - val_loss: 0.2796 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0958 - accuracy: 0.9697 - val_loss: 0.2946 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0994 - accuracy: 0.9689 - val_loss: 0.2916 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0982 - accuracy: 0.9678 - val_loss: 0.2883 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0994 - accuracy: 0.9689 - val_loss: 0.2911 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0950 - accuracy: 0.9694 - val_loss: 0.2865 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0910 - accuracy: 0.9709 - val_loss: 0.2809 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0933 - accuracy: 0.9706 - val_loss: 0.2999 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0923 - accuracy: 0.9701 - val_loss: 0.2872 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0921 - accuracy: 0.9712 - val_loss: 0.2907 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0964 - accuracy: 0.9689 - val_loss: 0.2831 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0917 - accuracy: 0.9719 - val_loss: 0.2833 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0939 - accuracy: 0.9698 - val_loss: 0.2907 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0912 - accuracy: 0.9708 - val_loss: 0.3055 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0932 - accuracy: 0.9688 - val_loss: 0.2839 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.2943 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0922 - accuracy: 0.9687 - val_loss: 0.3051 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.2939 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0884 - accuracy: 0.9721 - val_loss: 0.2837 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0888 - accuracy: 0.9714 - val_loss: 0.2886 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0890 - accuracy: 0.9716 - val_loss: 0.2952 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0906 - accuracy: 0.9708 - val_loss: 0.2950 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.3068 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0885 - accuracy: 0.9717 - val_loss: 0.2899 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0888 - accuracy: 0.9706 - val_loss: 0.3047 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "485/485 [==============================] - 8s 17ms/step - loss: 0.0841 - accuracy: 0.9726 - val_loss: 0.2932 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0869 - accuracy: 0.9716 - val_loss: 0.3037 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0872 - accuracy: 0.9707 - val_loss: 0.2994 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0866 - accuracy: 0.9719 - val_loss: 0.2959 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0846 - accuracy: 0.9717 - val_loss: 0.3019 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0838 - accuracy: 0.9719 - val_loss: 0.3038 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0856 - accuracy: 0.9723 - val_loss: 0.3021 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0849 - accuracy: 0.9733 - val_loss: 0.3050 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0862 - accuracy: 0.9723 - val_loss: 0.2835 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0829 - accuracy: 0.9732 - val_loss: 0.3068 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0839 - accuracy: 0.9728 - val_loss: 0.2991 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0853 - accuracy: 0.9722 - val_loss: 0.2975 - val_accuracy: 0.9257 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0822 - accuracy: 0.9736 - val_loss: 0.3114 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0838 - accuracy: 0.9725 - val_loss: 0.2896 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0799 - accuracy: 0.9739 - val_loss: 0.3114 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0801 - accuracy: 0.9741 - val_loss: 0.3114 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0820 - accuracy: 0.9743 - val_loss: 0.3054 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 0.3098 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0812 - accuracy: 0.9728 - val_loss: 0.3086 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0820 - accuracy: 0.9726 - val_loss: 0.3280 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0814 - accuracy: 0.9745 - val_loss: 0.3089 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0801 - accuracy: 0.9740 - val_loss: 0.3141 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0821 - accuracy: 0.9727 - val_loss: 0.3105 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0804 - accuracy: 0.9734 - val_loss: 0.3246 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0808 - accuracy: 0.9720 - val_loss: 0.3380 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0789 - accuracy: 0.9745 - val_loss: 0.2991 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0827 - accuracy: 0.9723 - val_loss: 0.3082 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0809 - accuracy: 0.9742 - val_loss: 0.3107 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "485/485 [==============================] - 8s 16ms/step - loss: 0.0782 - accuracy: 0.9734 - val_loss: 0.3204 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "485/485 [==============================] - 7s 15ms/step - loss: 0.0779 - accuracy: 0.9745 - val_loss: 0.3018 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "485/485 [==============================] - 7s 14ms/step - loss: 0.0809 - accuracy: 0.9719 - val_loss: 0.3004 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "[Finished Prtraining]\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "epcohs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "filepath = \"result/frequency_\" + dataset_name + \"_\" + time_string + \".h5\"\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=learning_rate*10)\n",
    "checkpoint = ModelCheckpoint(filepath, verbose = 0, monitor=\"val_loss\", mode=\"min\", save_best_only=True, save_weights_only = True)\n",
    "\n",
    "print(\"[Prtraninig Encoder with Supervised Contrastive Loss]\")\n",
    "model = Classifier(x_train.shape[1:], encoder, len(np.unique(y_train)), trainable=False)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "# history = model.fit(frequency_100_x_train, [frequency_100_y_train, contrastive_y_train], validation_data=(frequency_100_x_val, [frequency_100_y_val, contrastive_y_val]), batch_size=32, epochs=50, verbose=1, callbacks=[reduce_lr, checkpoint])\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch, epochs=epcohs, verbose=1, callbacks=[reduce_lr, checkpoint])\n",
    "print(\"[Finished Prtraining]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State :  4217067235\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'classified_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM 20220627.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRandom State : \u001b[39m\u001b[39m\"\u001b[39m, time_string)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m plot_training_acc(history, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m plot_training_loss(history, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.194.34.16/home/tolom/20220621/contrastive_frequency/CNN-BiLSTM%2020220627.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mclf()\n",
      "File \u001b[0;32m~/20220621/contrastive_frequency/utils.py:26\u001b[0m, in \u001b[0;36mplot_training_acc\u001b[0;34m(H, plotPath)\u001b[0m\n\u001b[1;32m     24\u001b[0m plt\u001b[39m.\u001b[39mstyle\u001b[39m.\u001b[39muse(\u001b[39m\"\u001b[39m\u001b[39mggplot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m---> 26\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39;49mhistory[\u001b[39m\"\u001b[39;49m\u001b[39mclassified_accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_classified_accuracy\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTraining accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classified_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Random State : \", time_string)\n",
    "plot_training_acc(history, None)\n",
    "plot_training_loss(history, None)\n",
    "plt.clf()\n",
    "print_test_result(model, filepath, x_test, y_test, y_test, dataset=dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "ajh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6413b6d85543380da3240e1425d765bd89a6aba645cf7b657591cceff44ffd45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
